{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# LANGUAGE TRANSLATION PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors: Nikhith Theddu, Sumanth Bhargav Kanchi, Hemalekha Pillarishetty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import collections\n",
    "import string\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "from numpy import array, argmax, random, take\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the data from txt file\n",
    "file = open('fra.txt','r')\n",
    "text = file.read()\n",
    "# using .split to grab the sentences\n",
    "sents = text.strip().split('\\n')\n",
    "sents = [i.split('\\t') for i in sents]\n",
    "for i in sents:\n",
    "    i.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['go', 'hi', 'hi', 'run', 'run'],\n",
       " ['va ', 'salut ', 'salut', 'coursâ€¯', 'courezâ€¯'],\n",
       " [1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = array(sents)\n",
    "# considering only 50000 sentences\n",
    "sentences = sentences[:50000,:]\n",
    "english = []\n",
    "french = []\n",
    "englishlen = []\n",
    "frenchlen = []\n",
    "for sent in sentences[:,0]:\n",
    "    # removing punctuations of English text\n",
    "    temp = sent.translate(str.maketrans('','',string.punctuation))\n",
    "    # converting text to lower\n",
    "    english.append(temp.lower())\n",
    "    englishlen.append(len(temp.split()))\n",
    "for sent in sentences[:,1]:\n",
    "    # removing punctuations of French text\n",
    "    temp = sent.translate(str.maketrans('','',string.punctuation))\n",
    "    # converting text to lower\n",
    "    french.append(temp.lower())\n",
    "    frenchlen.append(len(temp.split()))\n",
    "english[:5],french[:5],englishlen[:5],frenchlen[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls in English sentences is 0\n",
      "Nulls in French sentences is 0\n"
     ]
    }
   ],
   "source": [
    "# checking for nulls\n",
    "count = 0\n",
    "counts = 0\n",
    "for i in englishlen:\n",
    "    if i == 0:\n",
    "        count += 1\n",
    "for j in frenchlen:\n",
    "    if j == 0:\n",
    "        counts += 1\n",
    "print('Nulls in English sentences is',count)\n",
    "print('Nulls in French sentences is',counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(englishlen),max(frenchlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer function\n",
    "def tokens(text):\n",
    "      tokenizer = Tokenizer()\n",
    "        # assigns index \n",
    "      tokenizer.fit_on_texts(text)\n",
    "      return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6052, 14072)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tokenizer = tokens(english)\n",
    "eng_vocab = len(english_tokenizer.word_index) + 1\n",
    "french_tokenizer = tokens(french)\n",
    "french_vocab = len(french_tokenizer.word_index) + 1\n",
    "eng_vocab,french_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go</td>\n",
       "      <td>va</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hi</td>\n",
       "      <td>salut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi</td>\n",
       "      <td>salut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>run</td>\n",
       "      <td>coursâ€¯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>run</td>\n",
       "      <td>courezâ€¯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>who</td>\n",
       "      <td>qui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wow</td>\n",
       "      <td>ã‡a alorsâ€¯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fire</td>\n",
       "      <td>au feu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>help</td>\n",
       "      <td>ã€ laideâ€¯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jump</td>\n",
       "      <td>saute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stop</td>\n",
       "      <td>ã‡a suffitâ€¯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stop</td>\n",
       "      <td>stopâ€¯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>stop</td>\n",
       "      <td>arrãªtetoi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>wait</td>\n",
       "      <td>attends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>wait</td>\n",
       "      <td>attendez</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   english         french\n",
       "0       go            va \n",
       "1       hi         salut \n",
       "2       hi          salut\n",
       "3      run       coursâ€¯\n",
       "4      run      courezâ€¯\n",
       "5      who           qui \n",
       "6      wow   ã‡a alorsâ€¯\n",
       "7     fire        au feu \n",
       "8     help    ã€ laideâ€¯\n",
       "9     jump          saute\n",
       "10    stop  ã‡a suffitâ€¯\n",
       "11    stop        stopâ€¯\n",
       "12    stop    arrãªtetoi \n",
       "13    wait       attends \n",
       "14    wait      attendez "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating data frame\n",
    "df = pd.DataFrame({'english':english, 'french':french})\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "train, test = train_test_split(df, test_size=0.2, random_state = 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample English padded sequence is [  0   0   1 344 102 312 528]\n",
      "Sample French padded sequence is [   1   24   89 1063   11  535  756    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# converting text to sequences for train English data\n",
    "train_eng = english_tokenizer.texts_to_sequences(train['english'])\n",
    "# padding of the sequences for train English data\n",
    "train_eng = pad_sequences(train_eng, maxlen=max(englishlen), padding='pre')\n",
    "test_eng = english_tokenizer.texts_to_sequences(test['english'])\n",
    "test_eng = pad_sequences(test_eng, maxlen=max(englishlen), padding='pre')\n",
    "train_french = french_tokenizer.texts_to_sequences(train['french'])\n",
    "train_french = pad_sequences(train_french, maxlen=max(frenchlen), padding='post')\n",
    "test_french = french_tokenizer.texts_to_sequences(test['french'])\n",
    "test_french = pad_sequences(test_french, maxlen=max(frenchlen), padding='post')\n",
    "print('Sample English padded sequence is',train_eng[1])\n",
    "print('Sample French padded sequence is',train_french[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x286cd726780>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creation of a sequential model\n",
    "model = Sequential()\n",
    "model.add(Embedding(eng_vocab,512 , input_length=max(englishlen), mask_zero=True))\n",
    "model.add(LSTM(512))\n",
    "model.add(RepeatVector(max(frenchlen)))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dense(french_vocab, activation='softmax'))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate the model plot, due to some errors in installs, google collab is used for this piece of code\n",
    "#from keras.utils import plot_model\n",
    "#plot_model(model,to_file = 'model_1.png',show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building optimizer with learning rate 0.001\n",
    "rms = optimizers.RMSprop(lr=0.001)\n",
    "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.6399\n",
      "Epoch 00001: val_loss improved from inf to 2.04511, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 549s 9s/step - loss: 2.6399 - val_loss: 2.0451\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.0047\n",
      "Epoch 00002: val_loss improved from 2.04511 to 2.02790, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 548s 9s/step - loss: 2.0047 - val_loss: 2.0279\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9171\n",
      "Epoch 00003: val_loss improved from 2.02790 to 1.96878, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 565s 9s/step - loss: 1.9171 - val_loss: 1.9688\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.8409\n",
      "Epoch 00004: val_loss improved from 1.96878 to 1.87143, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 516s 8s/step - loss: 1.8409 - val_loss: 1.8714\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7561\n",
      "Epoch 00005: val_loss improved from 1.87143 to 1.79855, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 480s 8s/step - loss: 1.7561 - val_loss: 1.7986\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6636\n",
      "Epoch 00006: val_loss improved from 1.79855 to 1.71823, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 495s 8s/step - loss: 1.6636 - val_loss: 1.7182\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5821\n",
      "Epoch 00007: val_loss improved from 1.71823 to 1.65466, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 499s 8s/step - loss: 1.5821 - val_loss: 1.6547\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5098\n",
      "Epoch 00008: val_loss improved from 1.65466 to 1.61062, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 519s 8s/step - loss: 1.5098 - val_loss: 1.6106\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4460\n",
      "Epoch 00009: val_loss improved from 1.61062 to 1.56361, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 486s 8s/step - loss: 1.4460 - val_loss: 1.5636\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3881\n",
      "Epoch 00010: val_loss improved from 1.56361 to 1.53461, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 455s 7s/step - loss: 1.3881 - val_loss: 1.5346\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3327\n",
      "Epoch 00011: val_loss improved from 1.53461 to 1.49195, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 474s 8s/step - loss: 1.3327 - val_loss: 1.4919\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2779\n",
      "Epoch 00012: val_loss improved from 1.49195 to 1.47373, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 480s 8s/step - loss: 1.2779 - val_loss: 1.4737\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2263\n",
      "Epoch 00013: val_loss improved from 1.47373 to 1.44108, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 454s 7s/step - loss: 1.2263 - val_loss: 1.4411\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1759\n",
      "Epoch 00014: val_loss improved from 1.44108 to 1.40482, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 436s 7s/step - loss: 1.1759 - val_loss: 1.4048\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1282\n",
      "Epoch 00015: val_loss improved from 1.40482 to 1.37188, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 473s 8s/step - loss: 1.1282 - val_loss: 1.3719\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0830\n",
      "Epoch 00016: val_loss improved from 1.37188 to 1.35421, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 446s 7s/step - loss: 1.0830 - val_loss: 1.3542\n",
      "Epoch 17/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0382\n",
      "Epoch 00017: val_loss improved from 1.35421 to 1.32851, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 431s 7s/step - loss: 1.0382 - val_loss: 1.3285\n",
      "Epoch 18/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.9946\n",
      "Epoch 00018: val_loss improved from 1.32851 to 1.31430, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 414s 7s/step - loss: 0.9946 - val_loss: 1.3143\n",
      "Epoch 19/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.9536\n",
      "Epoch 00019: val_loss improved from 1.31430 to 1.29997, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 469s 7s/step - loss: 0.9536 - val_loss: 1.3000\n",
      "Epoch 20/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00020: val_loss improved from 1.29997 to 1.28598, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 628s 10s/step - loss: 0.9130 - val_loss: 1.2860\n",
      "Epoch 21/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.8745\n",
      "Epoch 00021: val_loss improved from 1.28598 to 1.26707, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 586s 9s/step - loss: 0.8745 - val_loss: 1.2671\n",
      "Epoch 22/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.8368\n",
      "Epoch 00022: val_loss improved from 1.26707 to 1.23872, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 589s 9s/step - loss: 0.8368 - val_loss: 1.2387\n",
      "Epoch 23/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.7991\n",
      "Epoch 00023: val_loss improved from 1.23872 to 1.21968, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 587s 9s/step - loss: 0.7991 - val_loss: 1.2197\n",
      "Epoch 24/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.7620\n",
      "Epoch 00024: val_loss improved from 1.21968 to 1.20798, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 585s 9s/step - loss: 0.7620 - val_loss: 1.2080\n",
      "Epoch 25/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.7292\n",
      "Epoch 00025: val_loss improved from 1.20798 to 1.19938, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 585s 9s/step - loss: 0.7292 - val_loss: 1.1994\n",
      "Epoch 26/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6946\n",
      "Epoch 00026: val_loss improved from 1.19938 to 1.18267, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 586s 9s/step - loss: 0.6946 - val_loss: 1.1827\n",
      "Epoch 27/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6622\n",
      "Epoch 00027: val_loss did not improve from 1.18267\n",
      "63/63 [==============================] - 575s 9s/step - loss: 0.6622 - val_loss: 1.1854\n",
      "Epoch 28/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6309\n",
      "Epoch 00028: val_loss improved from 1.18267 to 1.17750, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 590s 9s/step - loss: 0.6309 - val_loss: 1.1775\n",
      "Epoch 29/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6020\n",
      "Epoch 00029: val_loss improved from 1.17750 to 1.15363, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 601s 10s/step - loss: 0.6020 - val_loss: 1.1536\n",
      "Epoch 30/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.5726\n",
      "Epoch 00030: val_loss improved from 1.15363 to 1.14344, saving model to model.h1\n",
      "INFO:tensorflow:Assets written to: model.h1\\assets\n",
      "63/63 [==============================] - 628s 10s/step - loss: 0.5726 - val_loss: 1.1434\n"
     ]
    }
   ],
   "source": [
    "# saving model\n",
    "filename = 'model.h1'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# train model\n",
    "history = model.fit(train_eng, train_french.reshape(train_french.shape[0], train_french.shape[1], 1),\n",
    "                    epochs=30, batch_size=512, validation_split = 0.2,callbacks=[checkpoint], \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-b346d1a6978b>:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "# loading model\n",
    "model = load_model('model.h1')\n",
    "# predicting on test dataset\n",
    "predicted = model.predict_classes(test_eng.reshape((test_eng.shape[0],test_eng.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word match function\n",
    "def wor(n, tokenizer):\n",
    "      for word, index in tokenizer.word_index.items():\n",
    "          if index == n:\n",
    "              return word\n",
    "      return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract words using the word index \n",
    "preds = []\n",
    "for pred in predicted:\n",
    "    temp = []\n",
    "    for ele in range(len(pred)):\n",
    "        # checking for the index match with french tokenizer\n",
    "        words = wor(pred[ele], french_tokenizer)\n",
    "        if ele > 0:\n",
    "            if (words == wor(pred[ele-1], french_tokenizer)) or (words == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(words)\n",
    "        else:\n",
    "            if(words == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(words) \n",
    "    preds.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a data frame with English text, actual French and predicted French columns\n",
    "pred_df = pd.DataFrame({'English':test['english'],'actual' : test['french'], 'predicted' : preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating actual and predicted French text lists\n",
    "french_test = list(test['french'])\n",
    "actual = [] \n",
    "final = []\n",
    "for fre in french_test:\n",
    "    fre = nltk.word_tokenize(fre)\n",
    "    actual.append(fre)\n",
    "for pred in preds:\n",
    "    pred = nltk.word_tokenize(pred)\n",
    "    final.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\savit\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\savit\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\savit\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "# Bleu score calculation for each sentence\n",
    "for i in range(len(actual)):\n",
    "    temp = []\n",
    "    temp.append(actual[i])\n",
    "    score = sentence_bleu(temp, final[i],weights=(1, 0, 0, 0))\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4170232783948858"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average oF Bleu scores of all sentences\n",
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45835</th>\n",
       "      <td>they cant be ignored</td>\n",
       "      <td>ils ne peuvent ãªtre ignorã©s</td>\n",
       "      <td>ils ne peuvent pas virer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32973</th>\n",
       "      <td>theres no elevator</td>\n",
       "      <td>il ny a pas dascenseur</td>\n",
       "      <td>il ny a pas de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30800</th>\n",
       "      <td>i just want to help</td>\n",
       "      <td>je veux juste ãªtre utile</td>\n",
       "      <td>je veux juste que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8871</th>\n",
       "      <td>im devastated</td>\n",
       "      <td>je suis anã©antie</td>\n",
       "      <td>je suis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37537</th>\n",
       "      <td>ill not forget that</td>\n",
       "      <td>je ne loublierai pas</td>\n",
       "      <td>je noublierai pas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31266</th>\n",
       "      <td>i was happy for him</td>\n",
       "      <td>je fus heureux pour lui</td>\n",
       "      <td>jã©tais travaille pour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13381</th>\n",
       "      <td>tom is knocking</td>\n",
       "      <td>tom frappe ã  la porte</td>\n",
       "      <td>tom mã¨ne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20671</th>\n",
       "      <td>ill wait outside</td>\n",
       "      <td>jattendrai dehors</td>\n",
       "      <td>jattendrai ã</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43971</th>\n",
       "      <td>i work in the morning</td>\n",
       "      <td>je travaille le matin</td>\n",
       "      <td>je travaille dans au</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14278</th>\n",
       "      <td>are you japanese</td>\n",
       "      <td>ãštesvous japonais</td>\n",
       "      <td>ãštesvous malchanceuxâ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31048</th>\n",
       "      <td>i remember that guy</td>\n",
       "      <td>je me rappelle ce type</td>\n",
       "      <td>je me souviens de ce type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12190</th>\n",
       "      <td>im the captain</td>\n",
       "      <td>je suis le capitaine</td>\n",
       "      <td>je suis le</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12827</th>\n",
       "      <td>she never reads</td>\n",
       "      <td>elle ne lit jamais</td>\n",
       "      <td>elle ne jamais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27337</th>\n",
       "      <td>they finished 13th</td>\n",
       "      <td>ils ont terminã© 13e</td>\n",
       "      <td>ils ont fini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38562</th>\n",
       "      <td>please pull the rope</td>\n",
       "      <td>tire sur la corde je te prie</td>\n",
       "      <td>veuillez la</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     English                         actual  \\\n",
       "45835   they cant be ignored  ils ne peuvent ãªtre ignorã©s   \n",
       "32973     theres no elevator         il ny a pas dascenseur   \n",
       "30800    i just want to help      je veux juste ãªtre utile   \n",
       "8871           im devastated              je suis anã©antie   \n",
       "37537    ill not forget that           je ne loublierai pas   \n",
       "31266    i was happy for him        je fus heureux pour lui   \n",
       "13381        tom is knocking         tom frappe ã  la porte   \n",
       "20671       ill wait outside              jattendrai dehors   \n",
       "43971  i work in the morning          je travaille le matin   \n",
       "14278       are you japanese            ãštesvous japonais    \n",
       "31048    i remember that guy         je me rappelle ce type   \n",
       "12190         im the captain           je suis le capitaine   \n",
       "12827        she never reads             elle ne lit jamais   \n",
       "27337     they finished 13th           ils ont terminã© 13e   \n",
       "38562   please pull the rope   tire sur la corde je te prie   \n",
       "\n",
       "                                 predicted  \n",
       "45835    ils ne peuvent pas virer           \n",
       "32973              il ny a pas de           \n",
       "30800          je veux juste que            \n",
       "8871                   je suis              \n",
       "37537         je noublierai pas             \n",
       "31266    jã©tais travaille pour             \n",
       "13381                tom mã¨ne              \n",
       "20671            jattendrai ã               \n",
       "43971       je travaille dans au            \n",
       "14278  ãštesvous malchanceuxâ               \n",
       "31048    je me souviens de ce type          \n",
       "12190                je suis le             \n",
       "12827            elle ne jamais             \n",
       "27337              ils ont fini             \n",
       "38562              veuillez la              "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random 15 rows of the data frame\n",
    "pred_df.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a sequential model\n",
    "final_model = Sequential()\n",
    "final_model.add(Embedding(input_dim=eng_vocab,output_dim=512,input_length=max(englishlen),mask_zero = True))\n",
    "final_model.add(Bidirectional(GRU(512,return_sequences=False)))\n",
    "final_model.add(RepeatVector(max(frenchlen)))\n",
    "final_model.add(Bidirectional(GRU(512,return_sequences=True)))\n",
    "final_model.add(TimeDistributed(Dense(french_vocab,activation='softmax')))\n",
    "learning_rate = 0.001\n",
    "# compilation of the model\n",
    "final_model.compile(loss = 'sparse_categorical_crossentropy', \n",
    "                 optimizer = Adam(learning_rate), \n",
    "                 metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate the model plot, due to some errors in installs, google collab is used for this piece of code\n",
    "#from keras.utils import plot_model\n",
    "#plot_model(final_model,to_file = 'model_2.png',show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 3.1021 - accuracy: 0.6993 \n",
      "Epoch 00001: val_loss improved from inf to 2.21833, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 744s 12s/step - loss: 3.1021 - accuracy: 0.6993 - val_loss: 2.2183 - val_accuracy: 0.7164\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.1088 - accuracy: 0.7181 \n",
      "Epoch 00002: val_loss improved from 2.21833 to 2.05698, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 744s 12s/step - loss: 2.1088 - accuracy: 0.7181 - val_loss: 2.0570 - val_accuracy: 0.7209\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9775 - accuracy: 0.7276 \n",
      "Epoch 00003: val_loss improved from 2.05698 to 1.97029, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 796s 13s/step - loss: 1.9775 - accuracy: 0.7276 - val_loss: 1.9703 - val_accuracy: 0.7336\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.8598 - accuracy: 0.7385 \n",
      "Epoch 00004: val_loss improved from 1.97029 to 1.87185, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 774s 12s/step - loss: 1.8598 - accuracy: 0.7385 - val_loss: 1.8718 - val_accuracy: 0.7432\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7547 - accuracy: 0.7481 \n",
      "Epoch 00005: val_loss improved from 1.87185 to 1.80039, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 847s 13s/step - loss: 1.7547 - accuracy: 0.7481 - val_loss: 1.8004 - val_accuracy: 0.7523\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.6468 - accuracy: 0.7585 \n",
      "Epoch 00006: val_loss improved from 1.80039 to 1.70455, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 822s 13s/step - loss: 1.6468 - accuracy: 0.7585 - val_loss: 1.7045 - val_accuracy: 0.7626\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5221 - accuracy: 0.7694 \n",
      "Epoch 00007: val_loss improved from 1.70455 to 1.60979, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 935s 15s/step - loss: 1.5221 - accuracy: 0.7694 - val_loss: 1.6098 - val_accuracy: 0.7714\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.3988 - accuracy: 0.7788 \n",
      "Epoch 00008: val_loss improved from 1.60979 to 1.52935, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 865s 14s/step - loss: 1.3988 - accuracy: 0.7788 - val_loss: 1.5293 - val_accuracy: 0.7789\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2800 - accuracy: 0.7872 \n",
      "Epoch 00009: val_loss improved from 1.52935 to 1.45766, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 1051s 17s/step - loss: 1.2800 - accuracy: 0.7872 - val_loss: 1.4577 - val_accuracy: 0.7848\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.1747 - accuracy: 0.7940 \n",
      "Epoch 00010: val_loss improved from 1.45766 to 1.39517, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 829s 13s/step - loss: 1.1747 - accuracy: 0.7940 - val_loss: 1.3952 - val_accuracy: 0.7902\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0745 - accuracy: 0.8019 \n",
      "Epoch 00011: val_loss improved from 1.39517 to 1.33987, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 904s 14s/step - loss: 1.0745 - accuracy: 0.8019 - val_loss: 1.3399 - val_accuracy: 0.7949\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.9828 - accuracy: 0.8102\n",
      "Epoch 00012: val_loss improved from 1.33987 to 1.29295, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 676s 11s/step - loss: 0.9828 - accuracy: 0.8102 - val_loss: 1.2929 - val_accuracy: 0.7985\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.8963 - accuracy: 0.8189\n",
      "Epoch 00013: val_loss improved from 1.29295 to 1.25578, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 600s 10s/step - loss: 0.8963 - accuracy: 0.8189 - val_loss: 1.2558 - val_accuracy: 0.8039\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.8188 - accuracy: 0.8280\n",
      "Epoch 00014: val_loss improved from 1.25578 to 1.21795, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 626s 10s/step - loss: 0.8188 - accuracy: 0.8280 - val_loss: 1.2180 - val_accuracy: 0.8078\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.7468 - accuracy: 0.8384\n",
      "Epoch 00015: val_loss improved from 1.21795 to 1.18833, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 618s 10s/step - loss: 0.7468 - accuracy: 0.8384 - val_loss: 1.1883 - val_accuracy: 0.8104\n",
      "Epoch 16/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6837 - accuracy: 0.8482\n",
      "Epoch 00016: val_loss improved from 1.18833 to 1.16426, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 602s 10s/step - loss: 0.6837 - accuracy: 0.8482 - val_loss: 1.1643 - val_accuracy: 0.8131\n",
      "Epoch 17/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6279 - accuracy: 0.8578\n",
      "Epoch 00017: val_loss improved from 1.16426 to 1.14526, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 590s 9s/step - loss: 0.6279 - accuracy: 0.8578 - val_loss: 1.1453 - val_accuracy: 0.8150\n",
      "Epoch 18/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.5786 - accuracy: 0.8662\n",
      "Epoch 00018: val_loss improved from 1.14526 to 1.13625, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 600s 10s/step - loss: 0.5786 - accuracy: 0.8662 - val_loss: 1.1363 - val_accuracy: 0.8156\n",
      "Epoch 19/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.5379 - accuracy: 0.8740\n",
      "Epoch 00019: val_loss improved from 1.13625 to 1.12190, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 610s 10s/step - loss: 0.5379 - accuracy: 0.8740 - val_loss: 1.1219 - val_accuracy: 0.8189\n",
      "Epoch 20/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4976 - accuracy: 0.8818\n",
      "Epoch 00020: val_loss improved from 1.12190 to 1.11026, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 627s 10s/step - loss: 0.4976 - accuracy: 0.8818 - val_loss: 1.1103 - val_accuracy: 0.8215\n",
      "Epoch 21/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4631 - accuracy: 0.8892\n",
      "Epoch 00021: val_loss improved from 1.11026 to 1.10656, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 619s 10s/step - loss: 0.4631 - accuracy: 0.8892 - val_loss: 1.1066 - val_accuracy: 0.8227\n",
      "Epoch 22/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4336 - accuracy: 0.8946\n",
      "Epoch 00022: val_loss improved from 1.10656 to 1.10218, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 611s 10s/step - loss: 0.4336 - accuracy: 0.8946 - val_loss: 1.1022 - val_accuracy: 0.8231\n",
      "Epoch 23/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4074 - accuracy: 0.9005 \n",
      "Epoch 00023: val_loss improved from 1.10218 to 1.10114, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 753s 12s/step - loss: 0.4074 - accuracy: 0.9005 - val_loss: 1.1011 - val_accuracy: 0.8228\n",
      "Epoch 24/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3838 - accuracy: 0.9054\n",
      "Epoch 00024: val_loss improved from 1.10114 to 1.09924, saving model to model.h8\n",
      "INFO:tensorflow:Assets written to: model.h8\\assets\n",
      "63/63 [==============================] - 624s 10s/step - loss: 0.3838 - accuracy: 0.9054 - val_loss: 1.0992 - val_accuracy: 0.8251\n",
      "Epoch 25/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3629 - accuracy: 0.9095\n",
      "Epoch 00025: val_loss did not improve from 1.09924\n",
      "63/63 [==============================] - 586s 9s/step - loss: 0.3629 - accuracy: 0.9095 - val_loss: 1.1008 - val_accuracy: 0.8255\n",
      "Epoch 26/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3421 - accuracy: 0.9143\n",
      "Epoch 00026: val_loss did not improve from 1.09924\n",
      "63/63 [==============================] - 597s 9s/step - loss: 0.3421 - accuracy: 0.9143 - val_loss: 1.1013 - val_accuracy: 0.8263\n",
      "Epoch 27/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3231 - accuracy: 0.9185\n",
      "Epoch 00027: val_loss did not improve from 1.09924\n",
      "63/63 [==============================] - 591s 9s/step - loss: 0.3231 - accuracy: 0.9185 - val_loss: 1.1024 - val_accuracy: 0.8282\n",
      "Epoch 28/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3076 - accuracy: 0.9219\n",
      "Epoch 00028: val_loss did not improve from 1.09924\n",
      "63/63 [==============================] - 593s 9s/step - loss: 0.3076 - accuracy: 0.9219 - val_loss: 1.1072 - val_accuracy: 0.8281\n",
      "Epoch 29/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2937 - accuracy: 0.9245\n",
      "Epoch 00029: val_loss did not improve from 1.09924\n",
      "63/63 [==============================] - 591s 9s/step - loss: 0.2937 - accuracy: 0.9245 - val_loss: 1.1087 - val_accuracy: 0.8286\n",
      "Epoch 30/30\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2813 - accuracy: 0.9272\n",
      "Epoch 00030: val_loss did not improve from 1.09924\n",
      "63/63 [==============================] - 593s 9s/step - loss: 0.2813 - accuracy: 0.9272 - val_loss: 1.1163 - val_accuracy: 0.8283\n"
     ]
    }
   ],
   "source": [
    "# to save the model\n",
    "filename = 'model.h8'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# train model\n",
    "hist = final_model.fit(train_eng, train_french.reshape(train_french.shape[0], train_french.shape[1], 1), batch_size = 512, epochs = 30, validation_split = 0.2,callbacks=[checkpoint],verbose=1)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model\n",
    "final_model = load_model('model.h8')\n",
    "# predicting the test data\n",
    "final_predicted = final_model.predict_classes(test_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract words using the word index \n",
    "preds = []\n",
    "for pred in final_predicted:\n",
    "    temp = []\n",
    "    for ele in range(len(pred)):\n",
    "        # checking for the index match with french tokenizer\n",
    "        words = wor(pred[ele], french_tokenizer)\n",
    "        if ele > 0:\n",
    "            if (words == wor(pred[ele-1], french_tokenizer)) or (words == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(words)\n",
    "        else:\n",
    "            if(words == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(words) \n",
    "    preds.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a data frame with English text, actual French and predicted French columns\n",
    "pred_df = pd.DataFrame({'English':test['english'],'actual' : test['french'], 'predicted' : preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating actual and predicted French text lists\n",
    "french_test = list(test['french'])\n",
    "actual = [] \n",
    "final = []\n",
    "for fre in french_test:\n",
    "    fre = nltk.word_tokenize(fre)\n",
    "    actual.append(fre)\n",
    "for pred in preds:\n",
    "    pred = nltk.word_tokenize(pred)\n",
    "    final.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\savit\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\savit\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\savit\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Bleu score calculation for each sentence\n",
    "scores = []\n",
    "for i in range(len(actual)):\n",
    "    temp = []\n",
    "    temp.append(actual[i])\n",
    "    score = sentence_bleu(temp, final[i],weights=(1, 0, 0, 0))\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4683055177131358"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average of the scores\n",
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34196</th>\n",
       "      <td>who did you talk to</td>\n",
       "      <td>ã€ qui parlaistuâ€¯</td>\n",
       "      <td>ã€ qui parlaistuâ€¯ parlã©â€¯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8878</th>\n",
       "      <td>im farsighted</td>\n",
       "      <td>je vois loin</td>\n",
       "      <td>je suis affaiblie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38073</th>\n",
       "      <td>it was heartwarming</td>\n",
       "      <td>cã©tait rã©confortant</td>\n",
       "      <td>cã©tait fut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10990</th>\n",
       "      <td>give it to them</td>\n",
       "      <td>donnelaleur</td>\n",
       "      <td>faisle les</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43522</th>\n",
       "      <td>i often make mistakes</td>\n",
       "      <td>je commets souvent des fautes</td>\n",
       "      <td>je nombreuses souvent nombreuses erreurs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>tom did well</td>\n",
       "      <td>tom sest bien dã©brouillã©</td>\n",
       "      <td>tom a bien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11399</th>\n",
       "      <td>i dont need it</td>\n",
       "      <td>je nen ai pas besoin</td>\n",
       "      <td>je nen ai besoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18006</th>\n",
       "      <td>what a cute baby</td>\n",
       "      <td>quel mignon bã©bã©</td>\n",
       "      <td>quel est</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33823</th>\n",
       "      <td>we want to help tom</td>\n",
       "      <td>nous voulons aider tom</td>\n",
       "      <td>nous voulons suivre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41525</th>\n",
       "      <td>come over to my place</td>\n",
       "      <td>venez chez moi</td>\n",
       "      <td>allez ã   mon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37608</th>\n",
       "      <td>im glad i hired you</td>\n",
       "      <td>je suis heureux de vous avoir engagã©s</td>\n",
       "      <td>je suis content de tavoir engagã©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25114</th>\n",
       "      <td>i forced him to go</td>\n",
       "      <td>je lai forcã© ã  partir</td>\n",
       "      <td>je lai de  aller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37122</th>\n",
       "      <td>i think tom has gone</td>\n",
       "      <td>je pense que tom a disparu</td>\n",
       "      <td>je pense que tom est  parti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5674</th>\n",
       "      <td>i canceled it</td>\n",
       "      <td>je lai annulã©e</td>\n",
       "      <td>je lai annulã©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47169</th>\n",
       "      <td>whose turn is it next</td>\n",
       "      <td>ã€ qui le tourâ€¯</td>\n",
       "      <td>ã€ qui le  tourâ€¯</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     English                                  actual  \\\n",
       "34196    who did you talk to                     ã€ qui parlaistuâ€¯   \n",
       "8878           im farsighted                            je vois loin   \n",
       "38073    it was heartwarming                   cã©tait rã©confortant   \n",
       "10990        give it to them                             donnelaleur   \n",
       "43522  i often make mistakes           je commets souvent des fautes   \n",
       "4744            tom did well              tom sest bien dã©brouillã©   \n",
       "11399         i dont need it                    je nen ai pas besoin   \n",
       "18006       what a cute baby                     quel mignon bã©bã©    \n",
       "33823    we want to help tom                  nous voulons aider tom   \n",
       "41525  come over to my place                         venez chez moi    \n",
       "37608    im glad i hired you  je suis heureux de vous avoir engagã©s   \n",
       "25114     i forced him to go                 je lai forcã© ã  partir   \n",
       "37122   i think tom has gone              je pense que tom a disparu   \n",
       "5674           i canceled it                         je lai annulã©e   \n",
       "47169  whose turn is it next                       ã€ qui le tourâ€¯   \n",
       "\n",
       "                                               predicted  \n",
       "34196            ã€ qui parlaistuâ€¯ parlã©â€¯            \n",
       "8878                        je suis affaiblie             \n",
       "38073                            cã©tait fut              \n",
       "10990                             faisle les              \n",
       "43522  je nombreuses souvent nombreuses erreurs           \n",
       "4744                               tom a bien             \n",
       "11399                         je nen ai besoin            \n",
       "18006                               quel est              \n",
       "33823                     nous voulons suivre             \n",
       "41525                            allez ã   mon            \n",
       "37608          je suis content de tavoir engagã©          \n",
       "25114                          je lai de  aller           \n",
       "37122                 je pense que tom est  parti         \n",
       "5674                           je lai annulã©             \n",
       "47169                        ã€ qui le  tourâ€¯           "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random 15 rows of the data frame\n",
    "pred_df.sample(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
